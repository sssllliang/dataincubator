{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nlp_q2.ipynb NormalizedModel\n",
    "## @todo add tfidf normalization\n",
    "\n",
    "Review Input:\n",
    "NormalizedModel \n",
    "{u'text': u\"Love it!!!!! Love it!!!!!! love it!!!!!!!   Who doesn't love Culver's!\"}\n",
    "NormalizedModel \n",
    "{u'text': u'Everything was great except for the burgers they are greasy and very charred compared to other stores.'}\n",
    "NormalizedModel \n",
    "{u'text': u'I really like both Chinese restaurants in town.  This one has outstanding crab rangoon.  Love the chicken with snow peas and mushrooms and General Tso Chicken.  Food is always ready in 10 minutes which is accurate.  Good place and they give you free pop.'}\n",
    "NormalizedModel \n",
    "{u'text': u'Above average takeout with friendly staff. The sauce on the pan fried noodle is tasty. Dumplings are quite good.'}\n",
    "NormalizedModel \n",
    "{u'text': u\"We order from Chang Jiang often and have never been disappointed.  The menu is huge, and can accomodate anyone's taste buds.  The service is quick, usually ready in 10 minutes.\"}\n",
    "All questions successfully validated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 reviews\n"
     ]
    }
   ],
   "source": [
    "import simplejson as json\n",
    "import random\n",
    "\n",
    "lst_reviews = []\n",
    "lst_stars = []\n",
    "\n",
    "i = 0  # counter\n",
    "\n",
    "#lst_sample_mask = random.sample(range(1,1012913),3000)\n",
    "lst_sample_mask = random.sample(range(1,4000),3000)\n",
    "\n",
    "# build feature lists from random rows\n",
    "with open(\"/home/vagrant/miniprojects/questions/3_week/nlp/data/tiny_review.json\") as f:\n",
    "    for line in f:\n",
    "        if i in lst_sample_mask:\n",
    "            tmp = json.loads(line)\n",
    "            lst_reviews.append(tmp['text'])\n",
    "            lst_stars.append(tmp['stars'])\n",
    "        i += 1\n",
    "# reviews come out tokenized- each review is 1 string, but not\n",
    "# vectorized\n",
    "print len(lst_stars), \"reviews\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define stopwords ###\n",
    "\n",
    "# import nltk\n",
    "# stops = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# nltk corpus may not be available on heroku:\n",
    "stops = [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 14715)\n"
     ]
    }
   ],
   "source": [
    "# bag of words\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words_vectorizer = CountVectorizer(stops)\n",
    "\n",
    "counts = bag_of_words_vectorizer.fit_transform(lst_reviews)\n",
    "X_trn = counts\n",
    "print counts.shape\n",
    "\n",
    "# 5 reviews, 146 uniques words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## bag of with HashingVectorizer ###\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "bag_of_words_vectorizer = HashingVectorizer(n_features=int(1e5))\n",
    "\n",
    "counts = bag_of_words_vectorizer.fit_transform(lst_reviews)\n",
    "X_trn = counts\n",
    "print counts.shape\n",
    "\n",
    "# __ reviews, ___ unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_trn, lst_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl',\n",
       " '/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl_01.npy',\n",
       " '/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl_02.npy',\n",
       " '/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl_03.npy',\n",
       " '/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl_04.npy',\n",
       " '/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl_05.npy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(clf,'/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('/home/vagrant/miniprojects/questions/3_week/nlp/pickles/q1_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = clf.predict(X_trn) # define a transform method in clf to take raw strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 1 ..., 5 3 3]\n"
     ]
    }
   ],
   "source": [
    "print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
