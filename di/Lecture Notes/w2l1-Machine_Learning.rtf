{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Machine Learnin\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
http://vagrant:8888/notebooks/datacourse/lessons/Learning_and_Metrics.ipynb\
\
\
Machine learning involves 
\i training 
\i0 a model on a training data set, then unleashing that model on a 
\i testing
\i0  data set.\
\
X = \{X_ij\} is matrix of features (n x p)\
\
y = y_j is a vector of labels\
\
\
The hypothesis function maps a label y to a feature X_ij\
\
In 
\i supervised
\i0  learning:\
Our goal is to predict a label for some set of features.\
\
Regression = hypothesis function returns a real valued, continuous set of labels (y are scalars)\
\
Classification = hypothesis function returns discrete values: pass/fail, A,B,C,D, etc.\
\
Examples:\
\
Regression\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\
What will the volume of rainfall be\
\
Classification\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\
Will it rain tomorrow\
How many inches will it rain\
\
\

\i Unsupervised
\i0  Learning creates a mapping without knowing what the labels are\
\
Metrics\
\'97\'97\'97\'97\'97\'97\'97\
(R^2) Coefficient of Determination = Ratio of (sum of squared error) to (sum of mean error)\
\
Consider what preprocessing you need so that the metrics make sense\
\
(Squared error scales quadratically with y, Absolute error scales linearly, R^2 s indeterminate)\
\
Good f predictors increase the true positives & false negatives??\
\
\
Precision vs Recall\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
Means . . .\
High recall = high false postives\
High precision = high false negatives\
\
When deciding where to make the trade off consider the cost of a false positive vs false negatives.\
Ex, in terrorist screening, false positives are less costly than false negatives.\
\
For a spam filter, every false positive means you miss a good email, so high precision is better.\
\
\
Actually Doing It\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
Rather than checking all possible f \'97> precision vs recall, it\'92s easier to maximize the area under the curve of P vs R.\
\
The Receiver Operating Characteristic is also single valued\
\
The Log-Loss (entropy) \
\
\
Feature Importance\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
\
\
Parametric vs Non-parametric Analysis\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
If the underlying behavior is nonlinear, non-parametric is better??\
\
There is a tradeoff between goodness of the model, and its comprehensibility, especially\
if you don\'92t understand the underlying system.\
\
At worst you can run multiple models (excelling at different things) on different parts of your data set\
\
\
\
\
\
\
\
\
}