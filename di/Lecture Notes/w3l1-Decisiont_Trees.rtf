{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Decision Trees\
\'97\'97\'97\'97\'97\'97\'97\
At each step (branch) separate the remaining data based on some criteria. The criteria can change at each branch if you want??\
The value of the decision tree model at any branch, is the value of the terminating \'91leaf\'92\
\
*Interview question about decision trees are usually about understanding these parameters to minimize outlier influence: max_depth, min_samples_split, min_samples_leaf, max_features\
\
\
Because random forests are many independent decision trees, they scale very well via multiprocessing. Iterative models like Gradient Boosted Regressors scale very poorly as data gets big}