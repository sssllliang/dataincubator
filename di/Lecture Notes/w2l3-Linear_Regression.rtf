{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Linear Regression\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\

\i Gradient Descent = ??
\i0 \
\
To add an intercept, add a column of a constant to X_ij matrix\
\
Q3: If X has 3 collinear columns, You\'92ve increase the number of Beta that can optimally solve the problem. If you don\'92t know this, the regression answer won\'92t make sense.\
\
  If X has 3+ collinear columns, then 1 value of Beta will be weighted more heavily, and skew the results.\
\
  It also creates the illusion of more data points than you have.\
\
  In these cases you need to take a pseudo-inverse or something to get linear regression to work.\
\
Pearson coefficient describes the correlation between a feature and its label\
\
\

\i Regularization
\i0  is imposing a constraint on the minimization model.\
In Ridge Regression, the D_ii = 1/Sigma means that weights don\'92t scale equivalently; small weights scale down faster (per step), than large weights.\
\
When you have some regularization, your models become better at handling out-of-model error.\
\
Lasso Regression sets some feature weights to zero.\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
}