{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll see talk about \"scraping\": getting unstructured data and turning it into something usable.  The tools available through Python are mature and easy to use.  We'll focus primarily on _web scraping_, where the source data comes in HTML form.\n",
    "\n",
    "The basic workflow is:\n",
    "\n",
    "1.  Find the data you want on the web.\n",
    "2.  Inspect the page you're dealing with, to figure out how to zoom-in towards the content you want.  This will involve some combiation of\n",
    "    - Looking at the source code of the page (especially if it is simple), and\n",
    "    - Figuring out the structure of the HTML parse tree.  This step is much easier with a something like __Chrome Developer Tools__.\n",
    "3.  Write code to get out what you want:\n",
    "    - If the page is very simple, treat it as a bunch of text => __string manipulation / regular expressions__ in Python.\n",
    "    - If the page is more complicated (and/or written in good style), we want to use the HTML parse tree => __BeautifulSoup__ in Python.\n",
    "4.  Make sure it worked!\n",
    "5.  If your crawling problem is at all non-trivial, you will now have to go back to Step 2 to zoom in further -- or you'll have parsed the URL of a link you want to follow, in which case you'll go back to Step 1 to figure out how to parse what you want from the new target page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "As an example, suppose we want to crawl the list of \"Available Technologies\" being licensed by MIT at http://technology.mit.edu and store their basic info; their associated patents; and the reference counts on their associated patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Okay, let's go to that URL.\n",
    "\n",
    "- _First try_:  Aha, a list of links on the right.  Let's click on a few -- what do we see?  Many are empty, the categories are not obviously mutually exclusive, okay.  Maybe there's a better way.\n",
    "- _Second try_: Let's just search for all technologies at http://technology.mit.edu/technologies.  Okay, better but it only gives us 50 at a time.  We could just combine the four pages, that's fine.  Let's just click on page 2 to see what happens\n",
    "- _Third try_: Aha, the URL for page 2 is http://technology.mit.edu/technologies?limit=50&offset=50&query=.  That looks like we can just specify a higher limit and offset 0 and get the whole thing.\n",
    "- _Final answer_: Indeed, http://technology.mit.edu/technologies?limit=1000 has a giant list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "url = \"http://technology.mit.edu/technologies?limit=1000\"\n",
    "raw_page = urllib2.urlopen(url).read()\n",
    "print(raw_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A quick introduction to HTML and the DOM**\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Pull up http://technology.mit.edu/technologies?limit=1000 in Chrome.  \n",
    "- Open __View->Developer->Developer Tools__.  \n",
    "- Right click on one of the technology titles, and choose __\"Inspect Element\"__.\n",
    "\n",
    "What are we looking at?  Well.. it's this is the structure of the webpage.  Nested _tags_ of different _types_ and having a variety of _attributes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: What we learned above:\n",
    "\n",
    "  - All of the technologies are underneath (\"_descendents of_\")   `<div class=\"search\" id=\"nouvant-portfolio-content\">`\n",
    "  - In fact, each of them is in its own `<div class=\"technology\" data-images=\"true\" id=\"technology_XXXX\">`\n",
    "  \n",
    "Now we're ready to move on to **Step 3**: We'll use BeautifulSoup to leverage the above to zoom in on the individual technologies and to get links to the pages with detailed info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(raw_page)\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent_div = soup.find('div', attrs={'id': 'nouvant-portfolio-content'}) #Find (at most) *one*\n",
    "tech_divs = parent_div.find_all('div', attrs={'class':'technology'})  #Find *all*\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Introduction to CSS selectors**\n",
    "\n",
    "This pattern -- where you have nested finds, each given by conditions on tag type, id, and class -- is very common.  It's so common that there is a special convenience language for such traversals: [CSS selectors](http://www.w3schools.com/cssref/css_selectors.asp).\n",
    "\n",
    "BeautifulSoup supports a form of CSS selectors, and this will let us write the above in a more concise and expressive way:\n",
    "    >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "\n",
    "All selectors work like a 'find_all'.  Some basic building examples of selectors are:\n",
    "\n",
    " - _'mytag'_ picks out all tags of type _mytag_.\n",
    " - _'#myid'_ picks out all tags whose _id_ is equal to _myid_\n",
    " - _'.myclass'_ picks out all tags whose _class_ is equal to _myclass_\n",
    " - _'mytag#myid'_ will pick all tags of type _mytag_ **and** _id_ equal to _myid_ (analgously for _'mytag.myclass'_)\n",
    " - If _'selector1'_ and _'selector2'_ are two selectors, then there is another selector '_selector1 selector2'_.  It picks out all tags satisfying _selector2_ that are __descendents__(*) of something satisfying _selector1_, i.e., it's like our nested find.\n",
    " \n",
    " (*) It doesn't have to be a _direct_ descedent.  I.e., it can be a grand-grand-..-grand-child of something satisfying _selector1_.  For direct descendents we'd instead write _'selector1 > selector2'_\n",
    " \n",
    "Let's just explain how this applies to our example:\n",
    "\n",
    "1.  Let's start with the first half\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "This picks out all 'div' tags with id 'nouvant-portfolio-content'.\n",
    "2.  Then the second half\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                                                            ^^^^^^^^^^^^^^\n",
    "This picks out all 'div' tags with class 'technology'.\n",
    "3.  Finally the whole thing\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "does exactly the same as our nested find above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tech_divs = soup.select('div#nouvant-portfolio-content div.technology')\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what we've zoomed in to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print tech_divs[0].prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to pull out some key pieces of info:\n",
    "\n",
    "- The technology's \"title\" (the text in the `<a>` element)\n",
    "- The link to follow for more info on the technology (the _href_ attribute of the `<a>`)\n",
    "- And a short blurb about the text (in the `<span>`)\n",
    "\n",
    "Let's write some code to extract this.  But before we do, let's discuss what _form_ the output should take: It is often convenient to store data in _key-value_ form (e.g., as a hashtable), in other words to name the bits of data you are collecting.  One big advantage is that this makes it easier to add in extra fields progrssively.\n",
    "\n",
    "Let's see what the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "firsta = tech_divs[0].find('a')\n",
    "print firsta.text\n",
    "print firsta['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "# We're going to use a \"named tuple\" to store our key-value data.\n",
    "# We could also have used a dictionary, with strings as keys.\n",
    "# Named tuples have some advantages\n",
    "#  - Better notation, x.field_name instead of x['field_name']\n",
    "#  - If you change your object structure later and fail to update your\n",
    "#    code to include the new fields, this will make it easier to find.\n",
    "#  - They are immutable, preventing certain sorts of bugs.\n",
    "# .. and some disadvantages:\n",
    "#  - If you want to augment object structure you need a new type\n",
    "#    (or to go back and fill your code )\n",
    "#  - They are immutable.\n",
    "##\n",
    "from collections import namedtuple\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "\n",
    "def td_info(td):\n",
    "    la = td.select('h2 > a')\n",
    "    ls = td.select('span')\n",
    "    if len(la)!=1 or len(ls)!=1:\n",
    "        print \"Uh oh! We did something wrong\"\n",
    "        return None\n",
    "    return TechBasic (\n",
    "            title = la[0].text,\n",
    "            url   = la[0]['href'],\n",
    "            short = ls[0].text\n",
    "            )\n",
    "tech_links=[td_info(td) for td in tech_divs]\n",
    "\n",
    "print tech_links[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "def get_tech_details(tech_basic):\n",
    "    url_base=\"http://technology.mit.edu/\"\n",
    "    soup = BeautifulSoup( urllib2.urlopen(url_base + tech_basic.url) )\n",
    "    def patent_info(a):\n",
    "       return Patent ( \n",
    "                name = a.text, \n",
    "                url = a['href'] \n",
    "                )\n",
    "    patents = [patent_info(a) for a in soup.select('dd.us_patent_issued a')]\n",
    "    return TechDetailed ( \n",
    "            tech_basic = tech_basic, \n",
    "            patents = patents \n",
    "            )\n",
    "\n",
    "tech_basics = map(get_tech_details, tech_links[0:2])  #This takes a list\n",
    "print tech_basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "In the last code segment, we only did the first one.  If we try to get them all this way, it'll take a while.  Run the next cell for as long (or not) as you wish, and when you get bored use _Kernel->Interrupt_ to stop it.\n",
    "\n",
    "The problem is of course that it takes a while to connect to the remote server and fetch the page.  Fortunately, thought it takes a long time it is not actually _computationally expensive_: your computer would be perfectly happy doing this for 20 pages at a time.  The **multiprocessing** package in Python makes it easy to do this kind of (easy) parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slow version -- when I wrote this sheet, it took about 2 minutes to complete\n",
    "# Uncomment and run it to see\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# tech_details = map(get_tech_details, tech_links)  #This takes a list\n",
    "# end_time = time.time()\n",
    "\n",
    "# print \"Done!\", end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multi-processor version -- when I wrote this sheet, it took about 8 seconds to complete\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "workers = Pool(30)  # 30 worker processes\n",
    "\n",
    "start_time = time.time()\n",
    "tech_details = workers.map(get_tech_details, tech_links)\n",
    "end_time = time.time()\n",
    "\n",
    "print \"Done!\", end_time-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "Let's put all of that together.  Write a function \n",
    "```python\n",
    "def get_tech_basics(url):\n",
    "    ...\n",
    "```\n",
    "\n",
    "that returns `TechBasic` all each technology on the page.  Combine this with the pooled requests to get_tech_details to obtain a list of TechDetails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin.**\n",
    "That's it, we now have a basic not-entirely-trivial example.  Along the way we took some detours, so let's just take a look at what our code looks like without those detours:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "\n",
    "1. Modify \"get_tech_details\" to get other interesting information on the technology, like a long form description and/or the authors' names.  (You'll also want to modify TechDetailed.  Do that first and note that now the code breaks when it tries to construct a TechDetailed with the wrong number of fields.)\n",
    "\n",
    "2. Modify \"get_tech_details\" to try to follow the link and to get more information on the patent -- for instance when it was filed and granted, or how many other patents reference it.  (Warning: The patent web site is much less regular than MIT's!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coda / More complicated example**:\n",
    "Suppose we had picked Stanford instead of MIT.  Let's try to do the same thing (it's a bit harder to get a good listing URL, so I just downloaded one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from collections import namedtuple\n",
    "from multiprocessing import Pool\n",
    "\n",
    "raw_page=open('../small_data/Stanford-Tech-Listing.html', 'r')\n",
    "soup = BeautifulSoup(raw_page.read())\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BeautifulSoup doesn't seem to support 'or' selectors, so:\n",
    "tech_rows = soup.find_all(lambda x: x.has_attr('id') and x['id'].startswith('output_row'))[1:]\n",
    "# Alternate -- showing how to go up and down the tree\n",
    "#tech_rows = soup.find('tr', attrs={'id':'output_row_1'}).parent.findAll('tr')[1:]\n",
    "print len(tech_rows)\n",
    "print tech_rows[0].prettify()\n",
    "print tech_rows[-1].prettify()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details**: Let's quickly break down that last line for two bits of Python syntax that we haven't explicitly talked about\n",
    "    >    soup.find_all(lambda x: x.has_attr('id') and x['id'].startswith('output_row'))[1:]\n",
    "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                          ^^^^\n",
    "                                      (*)                                              (**)\n",
    "    * This is a \"lambda expression\" -- a short, inline, single-line, unnamed function.  In Python it has to be an *expression* (i.e., there's an implicit return out front) -- anything more complicated you have to define a named function. \n",
    "    * This is list slice notation (we already used this above with [0:1]!).  In this case, we're taking all but the zero-th entry (which is a list header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UH OH**: \n",
    "When originally preparing this, I was using Anaconda.  The same code only showed about _254_ of the _1727_ entries -- BeautifulSoup was incorrectly parsing the file.  These sorts of things are not entirely uncommon, so sometimes it helps to double-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This is hacky code!\n",
    "TechBlurb = namedtuple('TechBlurb', 'docket techid url title')\n",
    "def parse_tr(tr):\n",
    "    return TechBlurb(\n",
    "        docket = tr.select(\"td.output_data a\")[0].text,\n",
    "        techid = tr.select(\"td.output_data a\")[0]['href'].split(\"=\")[1],\n",
    "        url    = tr.select(\"td.output_data a\")[0]['href'],\n",
    "        title  = tr.select(\"td.output_data\")[2].text\n",
    "        )\n",
    "tech_blurbs=map(parse_tr, tech_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "# And this isn't much better!\n",
    "def find_comment_by_text_in(soup, comment_text):\n",
    "    return soup.find(text=lambda text: isinstance(text, Comment) and comment_text in text)\n",
    "\n",
    "TechDetailed = namedtuple('TechDetailed', 'blurb, abstract, similar')\n",
    "SimilarHint = namedtuple('SimilarHint', 'techid, docket, title')\n",
    "def get_tech_details(blurb):\n",
    "    # We're doing s lot of chaining with implicit assumptions here -- \n",
    "    #   it might fail in all sorts of way, in which case we give up.\n",
    "    try:\n",
    "        url_base=\"http://techfinder.stanford.edu/\"\n",
    "        soup = BeautifulSoup( urllib2.urlopen(url_base + blurb.url) )\n",
    "        contents = soup.find_all('form')[1]\n",
    "        abstract = find_comment_by_text_in(contents, 'Abstract').find_next_sibling('hr').find('div').text\n",
    "        similar = None\n",
    "        \n",
    "        def parse_similar_tr(r):\n",
    "                tds = r.find_all('td')\n",
    "                if len(tds) < 3:\n",
    "                    return None\n",
    "                return SimilarHint (\n",
    "                    techid = tds[0].find('a')['href'].split('=')[1], \n",
    "                    docket = \"S\"+tds[0].text.strip(), \n",
    "                    title  = tds[2].text.strip()\n",
    "                )\n",
    "        try:\n",
    "            similar_trs = find_comment_by_text_in(soup.find_all('form')[1], 'Similar Tech').find_next_sibling('table').find('div').find('table').find('table').find_all('tr')\n",
    "            similar = filter(None,[parse_similar_tr(tr) for tr in similar_trs])\n",
    "        except:\n",
    "            pass\n",
    "        return TechDetailed (\n",
    "            blurb    = blurb,\n",
    "            abstract = abstract,\n",
    "            similar  = similar\n",
    "        )\n",
    "    except:\n",
    "        return TechDetailed (\n",
    "            blurb   = blurb,\n",
    "            abstract = None,\n",
    "            similar = None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Since the point is to show that something goes wrong, let's not wait until the end!\n",
    "# imap_unordered lets you use the results of the map as they are produced (rather than storing them)\n",
    "# and with no guarantee on order.\n",
    "\n",
    "## This takes a while, so don't actually run it.\n",
    "#workers = Pool(30)  # 30 worker processes\n",
    "#tech_detailed = []\n",
    "#for r in workers.imap_unordered(get_tech_details, tech_blurbs):\n",
    "#    if r.similar is None:\n",
    "#        print \"Hmm, something is wrong with \", r.blurb\n",
    "#    tech_detailed.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "When we run the above code, it tells us that [this technology](http://techfinder.stanford.edu/technology_detail.php?ID=30261) did not have a list of similar technologies.  But going to the web page shows that it does!  What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url='http://techfinder.stanford.edu/technology_detail.php?ID=30261'\n",
    "soup = BeautifulSoup( urllib2.urlopen(url) )\n",
    "contents =soup.find_all('form')[1]\n",
    "print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go and look at the same part of the **raw** HTML, we find that there is no `</form>` there:\n",
    "\n",
    "    >    <!--- Applications --->\n",
    "    >    <h3>Applications</h3><br/>\n",
    "    >    <ul><li>Imaging apoptosis<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Research</li><li>Clinical<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Monitor therapeutic efficacy in cancer patients</li><li>Anti-cancer drug selection</ul></ul></li></ul><br/>\n",
    "    >    \n",
    "    >    <!--- Advantages --->\n",
    "    >    <h3>Advantages</h3><br/>\n",
    "    >    <ul><li>High specificity for caspase-3 and -7</li><li>High sensitivity</li><li>Non-invasive</li><li>Biocompatible</li><li>Small size of probe allows:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Deep tissue penetration</li><li>More extensive biodistribution</ul></li><li>PET probes:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>High tumor/muscle ratio in apoptotic tumors</li><li>High uptake value in apoptotic tumors</ul></li><li>Fluorescent probe:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Possess NIR spectral properties</ul></li><li>May help promote personalized cancer medicine</li><li>Potential for probe design strategy to be applied to other enzyme targets</li></ul><br/>\n",
    "\n",
    "What there **is** is _mal-formed HTML_ that is bad enough to confuse BeautifulSoup.  (Note that it's not nearly bad enough to confuse a web browser however).  If you look at more examples, you will find even worse ones -- a stray `</html>` in the middle of a document is not unheard of.  \n",
    "\n",
    "To fix this, we can pre-\"tidy\" the page before feeding it to BeautifulSoup using **pytidylib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tidylib import tidy_document\n",
    "url='http://techfinder.stanford.edu/technology_detail.php?ID=30261'\n",
    "\n",
    "tidy_page, _ = tidy_document(urllib2.urlopen(url).read())\n",
    "soup = BeautifulSoup(tidy_page)\n",
    "contents =soup.find_all('form')[1]\n",
    "print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "\n",
    "1. Go back and modify `get_tech_details` to use this 'tidy' approach.\n",
    "\n",
    "2. Sometimes web servers are slow and/or unreliable, and sometimes your connection it.  If we were to run the above test twice, we'd probably find that some of the failures were just due to a connection error.  We didn't notice this because the _outer_ `try` / `except` is also catching these.  So: Modify `get_tech_details` to allow up to 3 retries. <br/>Bonus points if you actually look what what exceptions `urllib` throws in those cases instead of a general catch-all mechanism.  Alternate type of bonus points if you figure out how to do it using the `retrying` package.  You can test these by throttling your internet on and off to simulate an unreliable connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Getting the list of short 'blurbs' about the techs\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "def get_tech_basics(url):\n",
    "    url = \"http://technology.mit.edu/technologies?limit=1000\"\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url))\n",
    "\n",
    "    ## Get the list of tech blurbs\n",
    "    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "\n",
    "    ## Parse a single 'td' on the index page\n",
    "    def td_info(td):\n",
    "        la = td.select('h2 > a')\n",
    "        ls = td.select('span')\n",
    "        if len(la)!=1 or len(ls)!=1:\n",
    "            print \"Uh oh! We did something wrong\"\n",
    "            return None\n",
    "        return TechBasic (\n",
    "                title = la[0].text,\n",
    "                url   = la[0]['href'],\n",
    "                short = ls[0].text\n",
    "                )\n",
    "    \n",
    "    return [td_info(td) for td in tech_divs]\n",
    "\n",
    "\n",
    "# Adding in some details (just patent info, for now)\n",
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "def get_tech_details(tech_basic):\n",
    "    url_base=\"http://technology.mit.edu/\"\n",
    "    soup = BeautifulSoup( urllib2.urlopen(url_base + tech_basic.url) )\n",
    "    def patent_info(a):\n",
    "       return Patent ( \n",
    "                name = a.text, \n",
    "                url = a['href'] \n",
    "                )\n",
    "    patents = [patent_info(a) for a in soup.select('dd.us_patent_issued a')]\n",
    "    return TechDetailed ( \n",
    "            tech_basic = tech_basic, \n",
    "            patents = patents \n",
    "            )\n",
    "\n",
    "## The main driver code:\n",
    "tech_basics = get_tech_basics(\"http://technology.mit.edu/technologies?limit=1000\")\n",
    "\n",
    "workers = Pool(30)  # 30 worker processes\n",
    "tech_details = workers.map(get_tech_details, tech_basics)\n",
    "\n",
    "print tech_details[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
