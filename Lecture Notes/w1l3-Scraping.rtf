{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 Scraping\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
import urllib2\
from bs4 import BeautifulSoup\
from collections import namedtuple\
from multiprocessing import Pool\
\
Find a way to search xml tags, ex: all h3 tags\
\
Study limit and offset in urls - standard ways of traversing linked lists\
	use these to programmitcally crawl all pages on a site\
\
xml tag classes and id\'92s are helpful for scraping too\
\
JQuery is helpful, you can also inject Jquery into any site (brandonmartinez)\
\
BeautifulSoup is a python lib for html & dom traversal\
\
from collections import namedtuple\
	namedtuple\'92s work like structs\
\
import time\
	start = time.time()\
	end = \'85\
\
To speed scraping code, we parallelize flops via \'91workers\'92\
	from multiprocessing import Pool\
	workers = Pool(30) #30 processes\
\
# slow 	myResult = map(scrpFcn, args) \
# fast		myResult = workers.map(scrpFcn, args) \
\
## python can\'92t multithread, so it multi-processes. This is expensive, but the best we can do.\
\
For massive crawls, python caches results to RAM any error will crash and delete all the results. Get around this with\
	try\
\
	accept\
OR \
Immediately write your results to disk\
\
\
\
\
Project\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
Weather ~ housing prices not correlated\
Stock market hedges\
}