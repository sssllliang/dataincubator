{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is Python's answer to R.  It's a good tool for small(ish) data analysis -- i.e., when everything fits into memory.\n",
    "\n",
    "The basic new \"noun\" in pandas is the **data frame**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun: \n",
    "\n",
    "### Data frames\n",
    "\n",
    "Like a table, with rows and columns (e.g., as in SQL).  Except:\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).  This is especially useful when you have timeseries data with potentially missing data points.\n",
    "  - Cells can store Python objects. Like in SQL, columns are type homogeneous.\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-cast\" integers to float64, etc.)\n",
    "  \n",
    "### Data Series:\n",
    "These are named columns of a DataFrame (more correctly, a dataframe is a dictionary of Series).  The entires of the series are have homogenous type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# a data frame\n",
    "df1 = pd.DataFrame({\n",
    "    'number': [1, 2, 3],\n",
    "    'animal': ['cat', 'dog', 'mouse']\n",
    "})\n",
    "\n",
    "# the same data frame\n",
    "df2 = pd.DataFrame([\n",
    "    ('cat', 1),\n",
    "    ('dog', 2),\n",
    "    ('mouse', 3),\n",
    "], columns=['animal', 'number'])\n",
    "\n",
    "assert((df1 == df2).all().all())\n",
    "\n",
    "# a series\n",
    "print df1['animal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbs: Operations\n",
    "  \n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "We'll go through a little on all of these in the context of an example.  To go through it, you must have the (output) data files from the HMDA \"Project structure\" example.  We're going to explore a dataset of mortgage insurance issued by the Federal Housing Authority (FHA).  The data is broken down by census tract and tells us how big of a player the FHA is in each tract (how many homes etc ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading data (and basic statistics / visualization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names =[\"State_Code\", \"County_Code\", \"Census_Tract_Number\", \"NUM_ALL\", \"NUM_FHA\", \"PCT_NUM_FHA\", \"AMT_ALL\", \"AMT_FHA\", \"PCT_AMT_FHA\"]\n",
    "df = pd.read_csv('../small_data/fha_by_tract.csv', names=names)  ## Loading a CSV file, without a header (so we have to provide field names)\n",
    "\n",
    "df['GEOID'] = df['Census_Tract_Number']*100 + 10**6 * df['County_Code'] \\\n",
    "    + 10**9 * df['State_Code']   ## A computed field!\n",
    "    \n",
    "df = df.sort('State_Code')  # sorting data to make it easier to read\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Basic statistics and a histogram of the percentage of mortages\"\n",
    "print \"in each census tract insured by FHA\"\n",
    "print df['PCT_AMT_FHA'].describe() \n",
    "df['PCT_AMT_FHA'].hist(bins=50, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The above distribution looks a little skewed, let's look at it's log\"\n",
    "print \"We can save off the data into a new column\"\n",
    "df['LOG_AMT_ALL'] = np.log(df['AMT_ALL'])\n",
    "print df['LOG_AMT_ALL'].describe()\n",
    "\n",
    "print \"We can use the apply function to transform data\"\n",
    "df['AMT_ALL'].apply(np.log).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Selecting off a column\"\n",
    "print df['State_Code'].head()\n",
    "print\n",
    "print \"Selecting off multiple columns\"\n",
    "print df[['State_Code', 'County_Code']].head()\n",
    "print\n",
    "print \"programatically access column names\"\n",
    "print all([col for col in df] == df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filtering\n",
    "\n",
    "Now the `df[...]` notation is very flexible:\n",
    "  - It accepts column names (strings and lists of strings);\n",
    "  - It accepts column numbers (so long as there is no ambiguity with column names..);\n",
    "  - It accepsts _binary data series!_\n",
    "  \n",
    "This means that you can write\n",
    "\n",
    "        df[ df['column_name2']==MD & ( df['column_name1']==5 | df['column_name1']==6 ) ]\n",
    "   \n",
    "for what you would write in SQL as\n",
    "\n",
    ">         SELECT * FROM df WHERE\n",
    "            column_name2='MD\" AND (column_name1=5 OR column_name1=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Selection returns a boolean array ..\"\n",
    "print (df['State_Code'] == 1).head()\n",
    "print\n",
    "print \"... we can apply the usual boolean operators to it\"\n",
    "print ((df['State_Code'] == 1) & (df['Census_Tract_Number'] == 9613)).head()\n",
    "print\n",
    "print \"pandas indices take boolean lists of the appropriate length\"\n",
    "print df[df['State_Code'] == 5].head()\n",
    "print\n",
    "print \"p.s. - numpy indices also take boolean arguments: here are some odd numbers\"\n",
    "print np.arange(10)[np.arange(10) % 2 == 1]\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "1. Plot the histogram of percentages for different states in the same graph to compare them.\n",
    "2. notice that there is a spike at 100%.  This means that the FHA has ensured 100% of the houses in that census tract.  See what happens to the histogram whem we restrct on the number of total loans is non-negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Join-ing\n",
    "\n",
    "The analogue of a\n",
    "\n",
    ">             \n",
    "    SELECT * \n",
    "        FROM df1\n",
    "        INNER JOIN df2 \n",
    "        ON df1.field_name=df2.field_name;\n",
    "\n",
    "is\n",
    "\n",
    "    df_joined = df1.merge(df2, on='field_name')\n",
    "\n",
    "You can also do left / right / outer joins, mix-and-match column names, etc.  For that consult the Pandas documentation. (The example below will do a left join..)\n",
    "\n",
    "Of course, just looking at the distribution of insurance by census tract isn't interesting unless we know more about the census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading information about census tracts\n",
    "df_geo = pd.read_csv('../small_data/2013_Gaz_tracts_national.tsv', sep='\\t')\n",
    "\n",
    "print \"joining two dataframes\"\n",
    "df_joined = df.merge(df_geo, on='GEOID', how='left')\n",
    "df_joined.sort('AMT_ALL', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "The analog of SQL's `GROUP BY` is\n",
    "\n",
    "    grouped = df.groupby(['field_name1', ...])...\n",
    "\n",
    "The above is analogous to\n",
    ">             \n",
    "    SELECT mean(df.value1), std(df.value2) \n",
    "        FROM df\n",
    "        GROUP BY df.field_name1, ...\n",
    "\n",
    "Pandas is somewhat more flexible in how you can use grouping, not requiring you to specify an aggregation function up front.  A few examples are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This isn't a SQL-style 'GROUP BY'.\n",
    "df_joined.groupby('USPS').first().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the analog of\n",
    "# SELECT USPS, SUM(AMT_FHA), SUM(AMT_ALL), ... FROM df GROUP BY USPS;\n",
    "df_by_state = df_joined.groupby('USPS')['AMT_FHA', 'AMT_ALL', 'NUM_FHA', 'NUM_ALL'].sum()\n",
    "\n",
    "df_by_state['PCT_AMT_FHA'] = 100.0 * df_by_state['AMT_FHA']  / df_by_state['AMT_ALL']\n",
    "df_by_state['PCT_NUM_FHA'] = 100.0 * df_by_state['NUM_FHA']  / df_by_state['NUM_ALL']\n",
    "\n",
    "# This sure looks different than the census-tract level histogram!\n",
    "df_by_state['PCT_AMT_FHA'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA handling\n",
    "\n",
    "\n",
    "When you read in a CSV file / SQL data base there are often \"NA\" (or \"null\", \"None\", etc.) values.  The CSV reader has a special field for specifying how this is denoted, and SQL has the built-in notion of NULL.  Pandas provides some tools for working with these -- they are generally similar too (and a little bit worse than) `R`\n",
    "\n",
    "- `isnull` / `notnull`: Testing for null-ness e.g., \n",
    ">       \n",
    "        df['column_name'].isnull()\n",
    "        \n",
    "   returns a Boolean series\n",
    "- `fillna`: Replacing null values by something else, e.g.,\n",
    ">         \n",
    "        df['column_name'].fillna(0)             # Fills constant value, here 0\n",
    "        df['column_name'].fillna(method='ffill')  # Fill forwards\n",
    "        df['column_name'].fillna(method='bfill', limit=5)  # Fill backwards, at most 5\n",
    "        \n",
    "    At least by default, this is *not in place* -- that is, it creates a new series and does not change the original one.\n",
    "\n",
    "- `interpolate`: Replacing null values by (linear, or quadratic, or...) interpolation.  There is support for indexing by times (not necessarily equally spaced), etc. in the documentation.  The most basic usage is\n",
    ">        \n",
    "        df['column_name'].interpolate()\n",
    "    \n",
    "    As above, this is not in place.\n",
    "\n",
    "\n",
    "For more details: http://pandas.pydata.org/pandas-docs/stable/missing_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas HTML data import example\n",
    "================================\n",
    "Pandas takes a \"batteries included approach\" and throws in a whole lot of convenience functions.  For instance it has import functions for a variety of formats.  One of the pleasant surprises is a command `read_html` that's meant to automate the process of extacting tabular data from HTML.  In particular, it works pretty well with tables on Wikipedia.  \n",
    "\n",
    "Let's do an example: We'll try to extract the list of the world's tallest buildings from\n",
    "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world\n",
    "\n",
    "\n",
    "(This example will, likely, not get lecture time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_html('http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world', header=0, parse_dates=False)\n",
    "\n",
    "# There are several tables on the page.  By inspection we can figure out which one we want\n",
    "tallest = dfs[2]  \n",
    "\n",
    "# The coordinates column needs to be fixed up.  This is a bit of string parsing:\n",
    "def clean_lat_long(s):\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "    except AttributeError:\n",
    "        return (None, None)\n",
    "    if len(parts)<3:\n",
    "        return None\n",
    "    m=re.search(r\"(\\d+[.]\\d+);[^\\d]*(\\d+[.]\\d+)[^\\d]\", parts[2])\n",
    "    if not m:\n",
    "        return (None, None)\n",
    "    return (m.group(1), m.group(2))\n",
    "\n",
    "tallest['Clean_Coordinates'] = tallest['Coordinates'].apply(clean_lat_long)\n",
    "tallest['Latitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[0])\n",
    "tallest['Longitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[1])\n",
    "\n",
    "# Et voila\n",
    "tallest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "-----------\n",
    "Unfortunately, that didn't work well on height (ft) and is so-so on year built.  \n",
    "\n",
    "In both cases, it is because it is assuming that the field is a _date_ rather than just a number.  We can hint these things to `read_html` through extra parameters.\n",
    "\n",
    "In this particular case we would probably not bother: It is pretty good on the year built, and height in meters is good enough.\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "1. Parse the height correctly.\n",
    "1. Parse the table rankings of [UK universities available on wikipedia](https://en.wikipedia.org/wiki/Rankings_of_universities_in_the_United_Kingdom):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Pivot table\" syntax, Stacking and unstacking\n",
    "\n",
    "These are useful tools -- and if you're aready familiar with Pivot Tables in e.g., Excel they'll be familiar.  Otherwise, this might be best saved for a hypothetical second lesson: see http://pandas.pydata.org/pandas-docs/stable/reshaping.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging into more advanced analytics\n",
    "-------\n",
    "Almost any \"advanced analytics\" tool in the Python ecosystem is going to take as input `np.array` type arrays.  You can access the underlying array of a data frame column as\n",
    "\n",
    "        df['column'].values\n",
    "        \n",
    "Many of them take `nd.array` whose underlying data can be accessed by \n",
    "\n",
    "        df.values\n",
    "        \n",
    "directly.  *Most* of the time, they will take `df['column']` and `df` without needing to look at values.\n",
    "\n",
    "This is particularly important if you want to use Pandas with the sklearn library. See this [blog post](http://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
